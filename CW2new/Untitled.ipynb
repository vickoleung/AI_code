{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modern-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from datasets import Flickr8k_Images, Flickr8k_Features\n",
    "from models import DecoderRNN, EncoderCNN\n",
    "from utils import *\n",
    "from config import *\n",
    "\n",
    "# if false, train model; otherwise try loading model from checkpoint and evaluate\n",
    "EVAL = False\n",
    "\n",
    "\n",
    "# reconstruct the captions and vocab, just as in extract_features.py\n",
    "lines = read_lines(TOKEN_FILE_TRAIN)\n",
    "image_ids, cleaned_captions = parse_lines(lines)\n",
    "vocab = build_vocab(cleaned_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bronze-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# initialize the models and set the learning parameters\n",
    "decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, len(vocab), NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complete-trauma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features torch.Size([7089, 2048])\n"
     ]
    }
   ],
   "source": [
    "features = torch.load('features.pt', map_location=device)\n",
    "print(\"Loaded features\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amber-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated features torch.Size([35445, 2048])\n"
     ]
    }
   ],
   "source": [
    "features = features.repeat_interleave(5, 0)\n",
    "print(\"Duplicated features\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "animal-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Flickr8k_Features(\n",
    "    image_ids=image_ids,\n",
    "    captions=cleaned_captions,\n",
    "    vocab=vocab,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "veterinary-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=64, # change as needed\n",
    "    shuffle=True,\n",
    "    num_workers=0, # may need to set to 0\n",
    "    collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rural-cycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35445\n",
      "35445\n",
      "torch.Size([35445, 2048])\n"
     ]
    }
   ],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=LR)\n",
    "\n",
    "print(len(image_ids))\n",
    "print(len(cleaned_captions))\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "functioning-fraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7089, 2048])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COMP5623M Coursework on Image Caption Generation\n",
    "\n",
    "\n",
    "Forward pass through Flickr8k image data to extract and save features from\n",
    "pretrained CNN.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from models import EncoderCNN\n",
    "from datasets import Flickr8k_Images\n",
    "from utils import *\n",
    "from config import *\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "lines = read_lines(TOKEN_FILE_TRAIN)\n",
    "# see what is in lines\n",
    "# print(lines[:2])\n",
    "\n",
    "#########################################################################\n",
    "#\n",
    "#       QUESTION 1.1 Text preparation\n",
    "# \n",
    "#########################################################################\n",
    "\n",
    "image_ids, cleaned_captions = parse_lines(lines)\n",
    "# to check the results after writing the cleaning function\n",
    "# print(image_ids[:2])\n",
    "# print(cleaned_captions[:2])\n",
    "\n",
    "vocab = build_vocab(cleaned_captions)\n",
    "# to check the results\n",
    "# print(\"Number of words in vocab:\", vocab.idx)\n",
    "\n",
    "# sample each image once\n",
    "image_ids = image_ids[::5]\n",
    "\n",
    "\n",
    "# crop size matches the input dimensions expected by the pre-trained ResNet\n",
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize(224), \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "dataset_train = Flickr8k_Images(\n",
    "    image_ids=image_ids,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EncoderCNN().to(device)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#\n",
    "#        QUESTION 1.2 Extracting image features\n",
    "# \n",
    "#########################################################################\n",
    "features = []\n",
    "\n",
    "\n",
    "# TODO loop through all image data, extracting features and saving them\n",
    "# no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs = data\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        features.append(outputs)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    features[i] = features[i].squeeze()\n",
    "    \n",
    "features = torch.cat(features, dim=0)\n",
    "\n",
    "# to check your results, features should be dimensions [len(train_set), 2048]\n",
    "# convert features to a PyTorch Tensor before saving\n",
    "print(features.shape)\n",
    "\n",
    "\n",
    "# save features\n",
    "torch.save(features, \"features.pt\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
